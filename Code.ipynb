{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "from openpyxl import load_workbook\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Four Differrent Ensemble Algorithms\n",
    "#1. Bagging - Random Forest\n",
    "#2. Boosting - Adapative Boosting, traditional Gradient Boosting (abondoned), Light GBM(better version of GBT), Extreme Gradient Boosting (Better version of Gradient Boosting)\n",
    "#3. Stacking - StackingCVRegressor(first layer - lgbm/xgb, svr, knn, meta - lasso/ridge)\n",
    "#4. Voting - Weighted Averaging, mean, median, VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G1 = C1, N2\n",
    "#G2 = C2, C3, C4, H2S\n",
    "#G3 = C6+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data import and re-organize\n",
    "df = pd.read_csv('C:/Users/cwu/OneDrive - EERC/Desktop/CO2 MMP ML/MMP_Impure_G1.csv')\n",
    "df.head()\n",
    "df2=np.array(df)\n",
    "\n",
    "Shuffled_data = shuffle(df, random_state = 25).reset_index().drop('index', axis=1) \n",
    "\n",
    "y_shuffled= Shuffled_data['MMP']\n",
    "x_shuffled=Shuffled_data.drop('MMP', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set and test set split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler().fit(x_shuffled)\n",
    "\n",
    "#skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=7)\n",
    "skf = ShuffleSplit(n_splits=5,test_size=0.25,random_state=7)\n",
    "for train_index,test_index in skf.split(x_shuffled,y_shuffled):\n",
    "    trainx1, testx1 = x_shuffled.iloc[train_index],x_shuffled.iloc[test_index]\n",
    "    trainy, testy = y_shuffled.iloc[train_index],y_shuffled.iloc[test_index]\n",
    "\n",
    "trainx = scaler.fit_transform(trainx1)\n",
    "testx = scaler.transform(testx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "# Grid Search\n",
    "random_seed=44\n",
    "random_forest_seed=np.random.randint(low=1,high=230)\n",
    "\n",
    "random_forest_hp_range={'n_estimators':[100,200,400,800,1600,1650,2000],\n",
    "                          'max_features':[10,11,12,13,14,15],\n",
    "                          'max_depth':[10,100,200,280,300,400],\n",
    "                          'min_samples_split':[2,3], # Greater than 1\n",
    "                          'min_samples_leaf':[1,2]\n",
    "                          }\n",
    "random_forest_model_test_base=RandomForestRegressor()\n",
    "random_forest_model_test_random=GridSearchCV(estimator=random_forest_model_test_base,\n",
    "                                               param_grid=random_forest_hp_range,\n",
    "                                               cv=5,\n",
    "                                               verbose=1,\n",
    "                                               n_jobs=-1,\n",
    "                                               scoring=\"neg_mean_squared_error\")\n",
    "grid_result_RF = random_forest_model_test_random.fit(trainx,trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_RF.best_score_, grid_result_RF.best_params_))\n",
    "\n",
    "means = grid_result_RF.cv_results_['mean_test_score']\n",
    "stds = grid_result_RF.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_RF.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "# Predict test set data\n",
    "\n",
    "best_model_RF = grid_result_RF.best_estimator_\n",
    "best_model_RF.fit(trainx,trainy)\n",
    "random_forest_predict=best_model_RF.predict(testx)\n",
    "random_forest_error=random_forest_predict-testy\n",
    "random_forest_relative_error=sum(abs(random_forest_predict-testy)/testy)*100/len(testx)\n",
    "\n",
    "# Draw test plot\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax=plt.axes(aspect='equal')\n",
    "plt.scatter(testy,random_forest_predict)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(\"Random Forest\")\n",
    "Lims=[0,100]\n",
    "plt.xlim(Lims)\n",
    "plt.ylim(Lims)\n",
    "plt.plot(Lims,Lims)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.hist(random_forest_error,bins=30)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Random Forest\")\n",
    "plt.grid(False)\n",
    "\n",
    "# Verify the accuracy\n",
    "\n",
    "random_forest_pearson_r=stats.pearsonr(testy,random_forest_predict)\n",
    "random_forest_R2=metrics.r2_score(testy,random_forest_predict)\n",
    "random_forest_RMSE=metrics.mean_squared_error(testy,random_forest_predict)**0.5\n",
    "print('Pearson correlation coefficient is {0}, RMSE is {1}, and r2 score is {2}.'.format(random_forest_pearson_r[0],\n",
    "                                                                        random_forest_RMSE,random_forest_R2))\n",
    "print('Relative Error is', random_forest_relative_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#ADABoost\n",
    "\n",
    "param_ADA = {\"estimator__max_depth\": [1,3,5,7,9],\n",
    "            \"estimator__min_samples_leaf\":[1,2,4,8,10],\n",
    "            \"estimator__max_leaf_nodes\":[5,10,20,40,80],\n",
    "            'n_estimators':[20,40,100,200],\n",
    "            'learning_rate':[0.001, 0.01, 0.1, 1.0]}       \n",
    "\n",
    "DTC = DecisionTreeRegressor()\n",
    "ADA_DT = AdaBoostRegressor(estimator=DTC,learning_rate=1,n_estimators=50,random_state=7)\n",
    "                           \n",
    "gsearch_ADADT = GridSearchCV(estimator = ADA_DT,param_grid = param_ADA,cv=5,scoring=\"neg_mean_squared_error\",n_jobs=-1)\n",
    "grid_result_ADADT = gsearch_ADADT.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_ADADT.best_score_, grid_result_ADADT.best_params_))\n",
    "\n",
    "means = grid_result_ADADT.cv_results_['mean_test_score']\n",
    "stds = grid_result_ADADT.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_ADADT.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set data\n",
    "\n",
    "best_model_ADADT = grid_result_ADADT.best_estimator_\n",
    "best_model_ADADT.fit(trainx,trainy)\n",
    "ADADT_predict=best_model_ADADT.predict(testx)\n",
    "ADADT_error=ADADT_predict-testy\n",
    "ADADT_relative_error=sum(abs(ADADT_predict-testy)/testy)*100/len(testx)\n",
    "\n",
    "# Draw test plot\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax=plt.axes(aspect='equal')\n",
    "plt.scatter(testy,ADADT_predict)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(\"AdaBoost\")\n",
    "Lims=[0,100]\n",
    "plt.xlim(Lims)\n",
    "plt.ylim(Lims)\n",
    "plt.plot(Lims,Lims)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.hist(ADADT_error,bins=30)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"AdaBoost\")\n",
    "plt.grid(False)\n",
    "\n",
    "# Verify the accuracy\n",
    "\n",
    "ADADT_pearson_r=stats.pearsonr(testy,ADADT_predict)\n",
    "ADADT_R2=metrics.r2_score(testy,ADADT_predict)\n",
    "ADADT_RMSE=metrics.mean_squared_error(testy,ADADT_predict)**0.5\n",
    "print('Pearson correlation coefficient is {0}, RMSE is {1}, and r2 score is {2}.'.format(ADADT_pearson_r[0],\n",
    "                                                                        ADADT_RMSE,ADADT_R2))\n",
    "print('Relative Error is', ADADT_relative_error) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBM\n",
    "\n",
    "parameters = {\n",
    "    'num_iterations': [100,500,1000],\n",
    "    'learning_rate':[0.01, 0.1, 1.0],\n",
    "    'num_leaves':[15,31,70],    \n",
    "    'max_depth' :[5,7,15],\n",
    "    'min_child_samples':[15,20,25],\n",
    "    'colsample_bytree': [0.6,0.8],\n",
    "    'subsample': [0.6,0.8],\n",
    "    'subsample_freq': [100,200,400]\n",
    "            }\n",
    "\n",
    "grid_LGBM = GridSearchCV(lgb.LGBMRegressor(), parameters, cv=5, n_jobs=-1,scoring=\"neg_mean_squared_error\")\n",
    "grid_result_LGBM = grid_LGBM.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_LGBM.best_score_, grid_result_LGBM.best_params_))\n",
    "\n",
    "means = grid_result_LGBM.cv_results_['mean_test_score']\n",
    "stds = grid_result_LGBM.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_LGBM.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set data\n",
    "\n",
    "best_model_LGBM = grid_result_LGBM.best_estimator_\n",
    "best_model_LGBM.fit(trainx,trainy)\n",
    "LGBM_predict=best_model_LGBM.predict(testx)\n",
    "LGBM_error=LGBM_predict-testy\n",
    "LGBM_relative_error=sum(abs(LGBM_predict-testy)/testy)*100/len(testx)\n",
    "\n",
    "# Draw test plot\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax=plt.axes(aspect='equal')\n",
    "plt.scatter(testy,LGBM_predict)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(\"LGBM\")\n",
    "Lims=[0,100]\n",
    "plt.xlim(Lims)\n",
    "plt.ylim(Lims)\n",
    "plt.plot(Lims,Lims)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.hist(LGBM_error,bins=30)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"LGBM\")\n",
    "plt.grid(False)\n",
    "\n",
    "# Verify the accuracy\n",
    "\n",
    "LGBM_pearson_r=stats.pearsonr(testy,LGBM_predict)\n",
    "LGBM_R2=metrics.r2_score(testy,LGBM_predict)\n",
    "LGBM_RMSE=metrics.mean_squared_error(testy,LGBM_predict)**0.5\n",
    "print('Pearson correlation coefficient is {0}, RMSE is {1}, and r2 score is {2}.'.format(LGBM_pearson_r[0],\n",
    "                                                                        LGBM_RMSE,LGBM_R2))\n",
    "print('Relative Error is', LGBM_relative_error)                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "parameters_XGB = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'learning_rate': [0.2,0.3,0.5,0.7], #so called `eta` value\n",
    "              'max_depth': [2,4,6],\n",
    "              'min_child_weight': [4,5],\n",
    "              'subsample': [i/10.0 for i in range(6,11)],\n",
    "              'colsample_bytree': [i/10.0 for i in range(6,11)],\n",
    "              'n_estimators': [50,100,250,500]}\n",
    "\n",
    "grid_XGB = GridSearchCV(XGBRegressor(), parameters_XGB, cv=5, n_jobs=-1,scoring=\"neg_mean_squared_error\")\n",
    "grid_result_XGB = grid_XGB.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_XGB.best_score_, grid_result_XGB.best_params_))\n",
    "\n",
    "means = grid_result_XGB.cv_results_['mean_test_score']\n",
    "stds = grid_result_XGB.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_XGB.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set data\n",
    "\n",
    "best_model_XGB = grid_result_XGB.best_estimator_\n",
    "best_model_XGB.fit(trainx,trainy)\n",
    "XGB_predict=best_model_XGB.predict(testx)\n",
    "XGB_error=XGB_predict-testy\n",
    "XGB_relative_error=sum(abs(XGB_predict-testy)/testy)*100/len(testx)\n",
    "\n",
    "# Draw test plot\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax=plt.axes(aspect='equal')\n",
    "plt.scatter(testy,XGB_predict)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(\"XGBoost\")\n",
    "Lims=[0,100]\n",
    "plt.xlim(Lims)\n",
    "plt.ylim(Lims)\n",
    "plt.plot(Lims,Lims)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.hist(XGB_error,bins=30)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"XGBoost\")\n",
    "plt.grid(False)\n",
    "\n",
    "# Verify the accuracy\n",
    "\n",
    "XGB_pearson_r=stats.pearsonr(testy,XGB_predict)\n",
    "XGB_R2=metrics.r2_score(testy,XGB_predict)\n",
    "XGB_RMSE=metrics.mean_squared_error(testy,XGB_predict)**0.5\n",
    "print('Pearson correlation coefficient is {0}, RMSE is {1}, and r2 score is {2}.'.format(XGB_pearson_r[0],\n",
    "                                                                        XGB_RMSE,XGB_R2))\n",
    "print('Relative Error is', XGB_relative_error)                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "parameters_SVR = {'kernel':['poly','rbf','sigmoid'], \n",
    "              'degree': [3,8], \n",
    "              'gamma': [1,0.1,0.01,0.001,0.0001],\n",
    "              'C': [0.1,1,10,100,1000]\n",
    "              }\n",
    "\n",
    "grid_SVR = GridSearchCV(SVR(), parameters_SVR, cv=5, n_jobs=-1,scoring=\"neg_mean_squared_error\")\n",
    "grid_result_SVR = grid_SVR.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_SVR.best_score_, grid_result_SVR.best_params_))\n",
    "\n",
    "means = grid_result_SVR.cv_results_['mean_test_score']\n",
    "stds = grid_result_SVR.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_SVR.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "parameters_KNN = {'n_neighbors':[i for i in range(1,11)], \n",
    "              'weights': ['uniform','distance'], \n",
    "              'p': [1,2]\n",
    "              }\n",
    "\n",
    "grid_KNN = GridSearchCV(KNeighborsRegressor(), parameters_KNN, cv=5, n_jobs=-1,scoring=\"neg_mean_squared_error\")\n",
    "grid_result_KNN = grid_KNN.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_KNN.best_score_, grid_result_KNN.best_params_))\n",
    "\n",
    "means = grid_result_KNN.cv_results_['mean_test_score']\n",
    "stds = grid_result_KNN.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_KNN.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "\n",
    "#meta = ridge, stacking\n",
    "\n",
    "best_model_SVR = grid_result_SVR.best_estimator_\n",
    "best_model_KNN = grid_result_KNN.best_estimator_\n",
    "\n",
    "xgb_best = best_model_XGB\n",
    "svr_best = best_model_SVR\n",
    "knn_best = best_model_KNN\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "stack = StackingCVRegressor(regressors=[xgb_best,svr_best,knn_best],\n",
    "                            meta_regressor = ridge,\n",
    "                            random_state = 7,\n",
    "                            use_features_in_secondary = True\n",
    "                            )\n",
    "\n",
    "#parameters_Stack = {'meta_regressor__penalty':['l1','l2'], \n",
    "              #'meta_regressor__C': [0.1,1,10,100] \n",
    "              #}\n",
    "\n",
    "parameters_Stack = {\n",
    "                    'meta_regressor':[ridge,lasso],\n",
    "                    'meta_regressor__alpha':[0.1,1,10]\n",
    "                    }\n",
    "\n",
    "grid_Stack = GridSearchCV(estimator=stack, \n",
    "                            param_grid = parameters_Stack,\n",
    "                            cv=5, \n",
    "                            n_jobs=-1,\n",
    "                            scoring=\"neg_mean_squared_error\")\n",
    "grid_result_Stack = grid_Stack.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_Stack.best_score_, grid_result_Stack.best_params_))\n",
    "\n",
    "means = grid_result_Stack.cv_results_['mean_test_score']\n",
    "stds = grid_result_Stack.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_Stack.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set data\n",
    "\n",
    "best_model_Stack = grid_result_Stack.best_estimator_\n",
    "best_model_Stack.fit(trainx,trainy)\n",
    "Stack_predict=best_model_Stack.predict(testx)\n",
    "Stack_error=Stack_predict-testy\n",
    "Stack_relative_error=sum(abs(Stack_predict-testy)/testy)*100/len(testx)\n",
    "\n",
    "# Draw test plot\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax=plt.axes(aspect='equal')\n",
    "plt.scatter(testy,Stack_predict)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(\"Stack_ridge\")\n",
    "Lims=[0,100]\n",
    "plt.xlim(Lims)\n",
    "plt.ylim(Lims)\n",
    "plt.plot(Lims,Lims)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.hist(Stack_error,bins=30)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Stack_ridge\")\n",
    "plt.grid(False)\n",
    "\n",
    "# Verify the accuracy\n",
    "\n",
    "Stack_pearson_r=stats.pearsonr(testy,Stack_predict)\n",
    "Stack_R2=metrics.r2_score(testy,Stack_predict)\n",
    "Stack_RMSE=metrics.mean_squared_error(testy,Stack_predict)**0.5\n",
    "print('Pearson correlation coefficient is {0}, RMSE is {1}, and r2 score is {2}.'.format(Stack_pearson_r[0],\n",
    "                                                                        Stack_RMSE,Stack_R2))\n",
    "print('Relative Error is', Stack_relative_error)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import preprocessing\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import max_norm\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learn_rate =0.01,neurons=12,dropoutrate=0.1):\n",
    "    neurons_2 = neurons/2\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=15,kernel_initializer='normal', activation='relu', kernel_constraint = max_norm(3)))\n",
    "    model.add(Dropout(dropoutrate))\n",
    "    model.add(Dense(neurons_2,kernel_initializer='normal', activation='relu', kernel_constraint=max_norm(3)))\n",
    "    model.add(Dropout(dropoutrate))\n",
    "    model.add(Dense(1,kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate = learn_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#create model\n",
    "model_1 = KerasRegressor(model=create_model,learn_rate=0.01,neurons=12,dropoutrate=0.1,verbose=0)\n",
    "\n",
    "#define the grid search parameters (epochs,batch size, optimizer)\n",
    "neurons = [12,16,32]\n",
    "batch_size = [20, 40, 80, 100]\n",
    "epochs = [50,100,150]\n",
    "learn_rate = [0.001,0.01,0.1]\n",
    "dropoutrate = [0.1,0.2,0.3,0.4,0.5]\n",
    "param_1 = dict(batch_size=batch_size, epochs=epochs,learn_rate = learn_rate,neurons=neurons,dropoutrate=dropoutrate)\n",
    "grid_1 = GridSearchCV(estimator=model_1,param_grid=param_1,cv=5,scoring=\"neg_mean_squared_error\",n_jobs=-1)\n",
    "grid_result_1 = grid_1.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_1.best_score_, grid_result_1.best_params_))\n",
    "\n",
    "means = grid_result_1.cv_results_['mean_test_score']\n",
    "stds = grid_result_1.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_1.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set data\n",
    "\n",
    "best_model_ANN = grid_result_1.best_estimator_\n",
    "best_model_ANN.fit(trainx,trainy)\n",
    "ANN_predict=best_model_RF.predict(testx)\n",
    "ANN_error=ANN_predict-testy\n",
    "ANN_relative_error=sum(abs(ANN_predict-testy)/testy)*100/len(testx)\n",
    "\n",
    "# Draw test plot\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax=plt.axes(aspect='equal')\n",
    "plt.scatter(testy,ANN_predict)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(\"ANN\")\n",
    "Lims=[0,100]\n",
    "plt.xlim(Lims)\n",
    "plt.ylim(Lims)\n",
    "plt.plot(Lims,Lims)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.hist(ANN_error,bins=30)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"ANN\")\n",
    "plt.grid(False)\n",
    "\n",
    "# Verify the accuracy\n",
    "\n",
    "ANN_pearson_r=stats.pearsonr(testy,ANN_predict)\n",
    "ANN_R2=metrics.r2_score(testy,ANN_predict)\n",
    "ANN_RMSE=metrics.mean_squared_error(testy,ANN_predict)**0.5\n",
    "print('Pearson correlation coefficient is {0},, RMSE is {1}, and r2 score is {2}.'.format(ANN_pearson_r[0],\n",
    "                                                                        ANN_RMSE,ANN_R2))\n",
    "print('Relative Error is', ANN_relative_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employ Softmax to calculated weight factors\n",
    "\n",
    "import math\n",
    "\n",
    "z = [random_forest_R2, ADADT_R2, LGBM_R2, XGB_R2, Stack_R2, ANN_R2]\n",
    "z_exp = [math.exp(i) for i in z]  \n",
    "print(z_exp) \n",
    "sum_z_exp = sum(z_exp)  \n",
    "print(sum_z_exp)  \n",
    "softmax = [round(i / sum_z_exp, 3) for i in z_exp]\n",
    "print(softmax)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "r1 = best_model_RF\n",
    "r2 = best_model_ADADT\n",
    "r3 = best_model_LGBM\n",
    "r4 = best_model_XGB\n",
    "r5 = best_model_Stack\n",
    "r6 = best_model_ANN\n",
    "\n",
    "er = VotingRegressor([('RF', r1),('ADA', r2), ('LGBM', r3),('XGB',r4),('Stack',r5),('ANN',r6)],weights=[1,1,1,1,1,1])\n",
    "\n",
    "parameters_er = {'weights':[(1,1,1,1,1,1),(softmax[0],softmax[1],softmax[2],softmax[3],softmax[4],softmax[5])]}\n",
    "\n",
    "grid_er = GridSearchCV(estimator=er, \n",
    "                        param_grid = parameters_er,\n",
    "                        cv=5, \n",
    "                        n_jobs=-1,\n",
    "                        scoring=\"neg_mean_squared_error\")\n",
    "grid_result_er = grid_er.fit(trainx, trainy)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result_er.best_score_, grid_result_er.best_params_))\n",
    "\n",
    "means = grid_result_er.cv_results_['mean_test_score']\n",
    "stds = grid_result_er.cv_results_['std_test_score']\n",
    "for mean,score,params in zip(means,stds,grid_result_er.cv_results_['params']):\n",
    "    print(\"%f (%f) with: %r\" % (mean, score, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set data\n",
    "\n",
    "best_model_er = grid_result_er.best_estimator_\n",
    "best_model_er.fit(trainx,trainy)\n",
    "er_predict=best_model_er.predict(testx)\n",
    "er_error=er_predict-testy\n",
    "er_relative_error=sum(abs(er_predict-testy)/testy)*100/len(testx)\n",
    "\n",
    "# Draw test plot\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax=plt.axes(aspect='equal')\n",
    "plt.scatter(testy,er_predict)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(\"ER\")\n",
    "Lims=[0,100]\n",
    "plt.xlim(Lims)\n",
    "plt.ylim(Lims)\n",
    "plt.plot(Lims,Lims)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.hist(er_error,bins=30)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"ER\")\n",
    "plt.grid(False)\n",
    "\n",
    "# Verify the accuracy\n",
    "\n",
    "er_pearson_r=stats.pearsonr(testy,er_predict)\n",
    "er_R2=metrics.r2_score(testy,er_predict)\n",
    "er_RMSE=metrics.mean_squared_error(testy,er_predict)**0.5\n",
    "print('Pearson correlation coefficient is {0}, RMSE is {1}, and r2 score is {2}.'.format(er_pearson_r[0],\n",
    "                                                                        er_RMSE,er_R2))\n",
    "print('Relative Error is', er_relative_error) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
